# -*- coding: utf-8 -*-
"""Faisal-CassvaDatasetChallnege-code_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXYHuUV5LM_MtYdq6abHiCWVhskNLjk9
"""

#Imports
import os
import sys
import glob
import torch
import torchvision

import numpy    as np
import datetime as dt
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot   as plt

from PIL               import Image
from torch.utils.data  import Dataset
from torch.autograd    import Variable
from torch.optim       import lr_scheduler

from torch.utils.data  import WeightedRandomSampler, Dataset, DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision       import transforms, datasets, models
from os                import listdir, makedirs, getcwd, remove
from os.path           import isfile, join, abspath, exists, isdir, expanduser

from torch import optim

# %matplotlib inline

# This assume that the data is contained in a google drive, you can nodify this according to where your data is stored
###########################################################################################
###########################################################################################
from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/MyDrive/ammi-2021-convnets"
train_path = join(data_path, "train/train")
test_path = join(data_path,"test/test")
extraimage_path = join(data_path, "extraimages/extraimages")
###########################################################################################
###########################################################################################

###########################################################################################
################# Transformations for both the training and testing data ##################
###########################################################################################
mean=[0.485, 0.456, 0.406]
std=[0.229, 0.224, 0.225]

normalize = transforms.Normalize(mean=mean, std=std)

# Do data transforms here, you can try many others
train_transforms = transforms.Compose([transforms.Resize(448),
                                       #transforms.RandomRotation(30),
                                       transforms.RandomHorizontalFlip(),
                                       transforms.RandomResizedCrop(448),
                                       transforms.ToTensor(),
                                       normalize,
                                      #  transforms.RandomErasing(),
                                      ])

test_transforms = transforms.Compose([ 
                                        transforms.Resize(448),
                                       #transforms.RandomRotation(30),
                                      #  transforms.RandomHorizontalFlip(),
                                       transforms.CenterCrop(448),
                                       transforms.ToTensor(),
                                       normalize,
                                     ])
###########################################################################################
###########################################################################################
###########################################################################################

###########################################################################################
############################# Dataset Class and data loading ##############################
###########################################################################################
class CassavaDataset(Dataset):
    def __init__(self, path, transform=None):
        self.classes = os.listdir(path)
        self.path = [f"{path}/{className}" for className in self.classes]
        self.file_list = [glob.glob(f"{x}/*") for x in self.path]
        self.transform = transform

        files = []
        for i, className in enumerate(self.classes):
            for fileName in self.file_list[i]:
                files.append([i, className, fileName])
        self.file_list = files
        files = None

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        fileName = self.file_list[idx][2]
        classCategory = self.file_list[idx][0]
        im = Image.open(fileName)
        if self.transform:
            im = self.transform(im)
            
        return im.view(3, 448, 448), classCategory

# Load the train and test set on a pytorch dataset object
train_data = CassavaDataset(train_path, transform=train_transforms)
test_data = CassavaDataset(test_path, transform=test_transforms)
print(f"size of training {len(train_data)}")
print(f"size of testing {len(test_data)}")

# split the data into training and validation.
import random
validation_split = .2
shuffle_dataset = True
random_seed= 42
BATCH_SIZE = 16

torch.manual_seed(random_seed)
random.seed(random_seed)
np.random.seed(random_seed)

# Creating data indices for training and validation splits:
dataset_size = len(train_data)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))

if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, val_indices = indices[split:], indices[:split]

# Creating pytorch data samplers and loaders:
torch.manual_seed(random_seed)
random.seed(random_seed)
np.random.seed(random_seed)

train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)


train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                             sampler=train_sampler)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                             sampler=valid_sampler)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)

# Device configuration
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print("device: {device}")
###########################################################################################
###########################################################################################
###########################################################################################

###########################################################################################
##################################### Useful methods ######################################
###########################################################################################
def train_one_batch(model, criterion, data_loader, optimizer, num_epochs):
    """
    This Method is used to train only on a one batch of the training set, the purpose of it is to make sure there is no bugs in the optimization loop.
    Method parameters:
    model: the model object.
    criterion: the loss function.
    data_loader: training data loader object.
    optmizer: the optimizer object.
    num_epochs: number of training epochs.
    The function returns: 
    losses: a list that containes the loss for every epoch.
    training_accuracies: a list that containes the traning accuracy for every epoch.
    """    
    batch = next(iter(data_loader))
    # Move model to the device (CPU or GPU).
    model = model.to(device)
    
    # Exponential moving average of the loss.
    ema_loss = None

    # Losses list 
    losses = []
    # training accuracy list
    training_accuracies = []

    print('----- Training Loop -----')
    # Loop over epochs.
    torch.manual_seed(random_seed)
    features, target = next(iter(data_loader))
    for epoch in range(num_epochs):
        # Make sure model is in training mode.
        model.train()  
        correct = 0
        # Loop over data.
        output = model(features.to(device))
        loss = criterion(output, target.to(device))

        # Backward pass.
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Compute the training accuracy
        with torch.no_grad():
            pred = output.argmax(dim=1, keepdim=True)

            # Count number of correct predictions.
            correct += pred.cpu().eq(target.view_as(pred)).sum().item()

        # NOTE: It is important to call .item() on the loss before summing.
        if ema_loss is None:
            ema_loss = loss.item()
        else:
            ema_loss += (loss.item() - ema_loss) * 0.01
        # calculate the accurcay on 

        # Print out progress the end of epoch.
        train_score = 100. * correct /  len(data_loader.sampler)
        losses.append(ema_loss)
        training_accuracies.append(train_score)
    print('Epoch: {} \tLoss: {:.12f} \tTraining Accuracy: {:.12f}'.format(epoch, ema_loss, train_score),)

    return losses, training_accuracies

def train(model, criterion, data_loader, valid_loader, optimizer, num_epochs, fixed_params_epochs = -1):
    """
    This Method runs the optimization loop for learning the parameters
    Method parameters:
    model: the model object.
    criterion: the loss function.
    data_loader: training data loader object.
    valid_loader: validation data loader object.
    optmizer: the optimizer object.
    num_epochs: number of training epochs.
    The function returns: 
    losses: a list that containes the loss for every epoch.
    training_accuracies: a list that containes the traning accuracy for every epoch.
    validation_accuracies: a list that containes the validation accuracy for every epoch.
    """
    # Move model to the device (CPU or GPU).
    model = model.to(device)
    
    # Exponential moving average of the loss.
    ema_loss = None

    # Losses list 
    losses = []
    # training acuecay list
    training_accuracies = []
    # valid acuecay list
    valid_acuecies = []

    print('----- Training Loop -----')
    # Loop over epochs.
    for epoch in range(num_epochs):
      # check the number of epochs to enable the gradient of the fearure extracter
      if epoch > fixed_params_epochs:
        for param in model.parameters():
          param.requires_grad = True
      # Make sure model is in training mode.
      model.train()  
      correct = 0
      # Loop over data.
      for batch_idx, (features, target) in enumerate(data_loader):
        # Forward pass.
        output = model(features.to(device))
        loss = criterion(output, target.to(device))
        
        # Backward pass.
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Compute the training accuracy
        with torch.no_grad():
          pred = output.argmax(dim=1, keepdim=True)
              
          # Count number of correct predictions.
          correct += pred.cpu().eq(target.view_as(pred)).sum().item()
      
      # NOTE: It is important to call .item() on the loss before summing.
        if ema_loss is None:
            ema_loss = loss.item()
        else:
            ema_loss += (loss.item() - ema_loss) * 0.01

      # Print out progress the end of epoch.
      lr_sch.step()
      train_score = 100. * correct / len(data_loader.sampler)
      # Compute the validation accuracy
      valid_score = test(model, valid_loader)
      losses.append(ema_loss)
      training_accuracies.append(train_score)
      validation_accuracies.append(valid_score)
      print('Epoch: {} \tLoss: {:.12f} \tTraining Accuracy: {:.12f} \t Validation Accuracy: {:.12f}'.format(epoch, ema_loss, train_score, valid_score),)

    return losses, training_accuracies, validation_accuracies

def test(model, data_loader, batch_size):
    """
    This Method runs measure the model accuracy on a given dataset
    Method parameters:
    model: the model object.
    data_loader: training data loader object.
    The function returns: 
    percent: the model accuracy
    """ 
    # Make sure the model is in evaluation mode.
    model.eval()
    correct = 0
    # print('----- Model Evaluation -----')
    # We do not need to maintain intermediate activations while testing.
    with torch.no_grad():
        # Loop over test data.
        # torch.manual_seed(random_seed)
        for features, target in data_loader:
          
            # Forward pass.
            output = model(features.to(device))
            
            # Get the label corresponding to the highest predicted probability.
            pred = output.argmax(dim=1, keepdim=True)
            
            # Count number of correct predictions.
            correct += pred.cpu().eq(target.view_as(pred)).sum().item()

    # Print test accuracy.
    percent = 100. * correct / len(data_loader.sampler)
    # print(f'Test accuracy: {correct} / {len(data_loader.dataset)} ({percent:.0f}%)')
    torch.save(model.state_dict(), 'model.ckpt')
    return percent

def plot(training_loss, training_score, valid_score):
    """
    Plot the model training metrics.
    Method parameters:
    training_loss: a list that containes the loss for every epoch.
    training_score: a list that containes the training accuracy for every epoch.
    valid_score: a list that containes the validation accuracy for every epoch.
    """
    epochs = range(1, len(training_loss)+1)
    plt.title("Training Loss")
    plt.plot(epochs, training_loss)
    plt.xlabel("Epochs")
    plt.ylabel("Training loss")

    plt.figure()
    plt.plot(epochs, training_score, label="Train")
    plt.plot(epochs, valid_score, label="Validation")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
###########################################################################################
###########################################################################################
###########################################################################################

"""# Simple Model"""

# Expermentation with a simple model
class Classifier(nn.Module):
    def __init__(self, num_classes):
        super(Classifier, self).__init__()
        # Block 1
        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 5, stride = 2, padding = 1)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)

        #Block 2
        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 2)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        #Block 3
        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 2)
        self.relu3 = nn.ReLU()
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)

        # last fully-connected layer
        self.fc = nn.Linear(32*3*3, num_classes)


    def forward(self, input):

        x = self.maxpool1(self.relu1(self.conv1(input)))
        x = self.maxpool2(self.relu2(self.conv2(x)))
        x = self.maxpool3(self.relu3(self.conv3(x)))

        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

###########################################################################################
####################### basic input indepedent baseline (all zeros) #######################
###########################################################################################
torch.manual_seed(random_seed) # fix the random seed
model = Classifier(5)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
num_epochs = 10
losses, train_score, valid_score = train(model, criterion, train_loader, valid_loader ,optimizer, num_epochs, BATCH_SIZE, iszeros=True)

plot(losses, train_score, valid_score) # plot the training dynamics
###########################################################################################
###########################################################################################

###########################################################################################
######## overfit one batch to make sure there is no bugs in the optimization loop #########
###########################################################################################
torch.manual_seed(random_seed)
model = Classifier(5)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
num_epochs = 5000
loess, train_scores = train_one_batch(model, criterion, train_loader, optimizer, num_epochs, BATCH_SIZE)

plot(loess, train_scores, train_scores) # plot the training dynamics
###########################################################################################
###########################################################################################

###########################################################################################
######################### Train the model on the whole data set ###########################
###########################################################################################
torch.manual_seed(random_seed)
model = Classifier(5)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
num_epochs = 10
loess, train_scores = train(model, criterion, train_loader, valid_loader ,optimizer, num_epochs, BATCH_SIZE)

plot(loess, train_scores, train_scores) # plot the training dynamics
###########################################################################################
###########################################################################################

"""# ResNet101"""

###########################################################################################
############### Train a pretrained resnet101 model on the whole data set ##################
###########################################################################################
torch.manual_seed(random_seed)
model = torchvision.models.resnet101(pretrained=True)

# set the final linear layer to the number of classes of the problem
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 5)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay = 5e-3)
num_epochs = 80
losses, train_score, valid_score= train(model, criterion, train_loader, valid_loader ,optimizer, num_epochs, BATCH_SIZE)
# Best validation accuecay was about 73% while the training accuracy was 86.5%, the model is overfitting

plot(losses, train_score, valid_score) # plot the training dynamics
###########################################################################################
###########################################################################################

"""# ResNest50"""

###########################################################################################
############### Train a pretrained resnext50 model on the whole data set ##################
###########################################################################################
torch.manual_seed(random_seed) # fix the random seed
model = torchvision.models.resnext50_32x4d(pretrained=True)

# for param in model.parameters():
#     param.requires_grad = False

# set the final linear layer to the number of classes of the problem
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 5)
model = model.to(device)
criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=3e-3)
# a cosine Learning rate scheduler
lr_sch = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1, verbose=False)
num_epochs = 7
losses, train_score, valid_score= train(model, criterion, train_loader, valid_loader ,optimizer, num_epochs)
# Best validation accuecay was about 89.8% while the training accuracy was 91.2%, the model is overfitting

plot(losses, train_score, valid_score) # plot the training dynamics
###########################################################################################
###########################################################################################

# A dictionary that maps the labels to the classes names
classes_dict = {i:class_ for i, class_ in zip(range(5), train_data.classes)}

# precition function
def predict(model, test_loader):
  model.eval()
  with torch.no_grad():
    preds = []
    # i = 0
    for img, label in test_loader:
      # i += 1
      outputs = model(torch.unsqueeze(img.to(device=device), dim=0))
      pred = torch.argmax(outputs, dim=1)
      preds.append(classes_dict[pred.item()])
  return preds

model = torchvision.models.resnext50_32x4d(pretrained=True) # define your model architecture
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 5)
model = model.to(device)

model.load_state_dict(torch.load("model.ckpt")) # load the saved model state, which includes the final parameters
preds = predict(model, test_data)

filenames = [filename[2].split('/')[-1] for filename in test_data.file_list] # extract the filenames

# make a submission file in the current directory
import pandas as pd
submission = (pd.DataFrame.from_dict({'Category': preds,'Id': filenames}))
sub_file = "first_submission.csv"
submission.to_csv(sub_file, header=True, index=False)